## Implementare TEMA1-METODE NUMERICE:

### Task 1. Markov is coming...

1. Functia parse_labyrinth:
	Am citit prima linie din fiÈ™ier cu [fgetl]
(https://www.mathworks.com/help/matlab/ref/fgetl.html) È™i am folosit
[str2num] (https://www.mathworks.com/help/matlab/ref/str2num.html) pentru a
extrage dimensiunile matricei (m È™i n). Apoi, cu ajutorul unui while, am parcurs
fiecare linie a fiÈ™ierului, am convertit-o Ã®n vector numeric tot cu str2num È™i
am completat rÃ¢ndurile matricei Labyrinth. Metoda de procesare a fost inspirata
din primul laborator.
	AceastÄƒ abordare este simplÄƒ È™i eficientÄƒ pentru fiÈ™iere structurate corect.
Singura verificare inclusÄƒ este pentru deschiderea fiÈ™ierului, unde se afiÈ™eazÄƒ
un mesaj de eroare dacÄƒ acesta nu poate fi accesat.

2. Functia get_adjacency_matrix:
	Am extras dimensiunile labirintului cu [size]
(https://www.mathworks.com/help/matlab/ref/double.size.html), apoi am construit
o matrice matrix cu valori de la 1 la m*n, folosind [reshape]
(https://www.mathworks.com/help/matlab/ref/double.reshape.html) È™i douÄƒ for-uri.
Am iniÈ›ializat matricile win_matrix È™i lose_matrix pentru a marca stÄƒrile
speciale. Fiecare element al labirintului a fost convertit Ã®n binar cu [dec2bin]
(https://www.mathworks.com/help/matlab/ref/dec2bin.html), rezultÃ¢nd
un string. Pentru a obÈ›ine valorile [b3 b2 b1 b0], am folosit str2num pe fiecare
caracter. Fiecare bit reprezintÄƒ existenÈ›a unui perete: Nord (b3), Sud (b2), Est
(b1), Vest (b0). DacÄƒ pentru b3 sau b2 suntem pe prima sau ultima linie, nodul
curent se leagÄƒ de starea WIN (indice m x n+1). Ãn caz contrar, se creeazÄƒ o
legÄƒturÄƒ cu nodul de deasupra sau dedesubt. Analog, pentru b1 È™i b0, dacÄƒ ne
aflÄƒm pe marginea din stÃ¢nga sau dreapta, nodul se leagÄƒ de starea LOSE
(m x n+2). Ãn rest, se conecteazÄƒ cu vecinii din stÃ¢nga/dreapta. Am adÄƒugat
legÄƒturi Ã®ntre stÄƒrile WIN È™i LOSE, iar la final am creat matricea de adiacenÈ›Äƒ
de tip [sparse] (https://docs.octave.org/v4.2.2/Creating-Sparse-Matrices.html)
pentru a optimiza spaÈ›iul de memorie ocupat. Pentru logica construirii reÈ›elei
È™i a matricii de adiacenÈ›Äƒ, m-am inspirat din doua exemple MATLAB disponibile
aici: https://ece.uwaterloo.ca/~dwharder/Matlab/indexing.html si
https://www.mathworks.com/matlabcentral/answers/2002377-i-want-to-get-adjacency-matrix-of-a-networkz

3. Functia get_link_matrix:
	Pornind de la matricea de adiacenÈ›Äƒ Adj, am construit matricea de legÄƒturÄƒ
Link, unde fiecare element nenul de pe o linie este Ã®mpÄƒrÈ›it la numÄƒrul total de
elemente nenule de pe linia respectivÄƒ. AceastÄƒ normalizare transformÄƒ matricea
Ã®ntr-o formÄƒ probabilisticÄƒ, utilÄƒ pentru modelÄƒri de tip lanÈ› Markov. Pentru a
calcula suma valorilor de pe fiecare linie, am folosit funcÈ›ia [sum]
(https://www.mathworks.com/help/matlab/ref/double.sum.html) din
documentaÈ›ia oficialÄƒ MATLAB. OperaÈ›ia a fost realizatÄƒ Ã®n cadrul unui for care
parcurge liniile È™i coloanele, Ã®mpÄƒrÈ›ind fiecare valoare 1 la suma totalÄƒ a
liniei respective. Matricea rezultatÄƒ este de tip sparse pentru a optimiza
utilizarea memoriei.

4. Functia get_Jacobi_parameters:
	Extragem matricea È™i vectorul de iteraÈ›ii necesare pentru aplicarea metodei
Jacobi pe modelul de labirint. Pentru aceasta, am eliminat ultimele douÄƒ linii
È™i coloane din matricea de legÄƒturÄƒ Link, corespunzÄƒtoare stÄƒrilor speciale WIN
È™i LOSE. Am construit matricea G folosind primele n1 linii È™i n2 coloane, iar
vectorul c a fost extras din coloana n2+1, reprezentÃ¢nd influenÈ›a stÄƒrii WIN
asupra fiecÄƒrui nod din reÈ›ea.

5. Functia perform_iterative:
	Am implementat metoda Jacobi pentru rezolvarea sistemului ğ‘¥=ğºğ‘¥+ğ‘, iterativ.
IniÈ›ial, x â† x0, err â† tol + 1, iar la fiecare pas se salveazÄƒ vechea soluÈ›ie
(x_anterior) pentru a calcula norma diferenÈ›ei. DacÄƒ eroarea devine mai micÄƒ
decÃ¢t tol sau se atinge max_steps, procesul se opreÈ™te. FuncÈ›ia Ã®ntoarce soluÈ›ia
finalÄƒ, eroarea È™i numÄƒrul de paÈ™i executaÈ›i. Algoritmul este eficient dacÄƒ
matricea G respectÄƒ condiÈ›iile de convergenÈ›Äƒ. Metoda de rezolvare a
algoritmului Jacobi este la fel ca cea facuta la laborator.

6. Functia heuristic_greedy:
	Am implementat un algoritm greedy euristic pentru a gÄƒsi un traseu dintr-o
poziÈ›ie iniÈ›ialÄƒ cÄƒtre starea WIN, pe baza probabilitÄƒÈ›ilor. La fiecare pas, cu
[find] (https://www.mathworks.com/help/matlab/ref/find.html)
alegem vecinul nevizitat cu probabilitatea maximÄƒ. DacÄƒ nu existÄƒ vecini
eligibili [isempty]
(https://www.mathworks.com/help/matlab/ref/double.isempty.html), facem
backtracking. Se evitÄƒ ciclurile cu un vector visited, iar algoritmul se opreÈ™te
la atingerea stÄƒrii WIN.

7. Functia decode_path:
	Am implementat o funcÈ›ie care transformÄƒ un vector de stÄƒri Ã®ntr-un traseu
exprimat prin coordonate (linie, coloanÄƒ). Am reconstruit o matrice de referinÈ›Äƒ
pentru a lega fiecare stare de poziÈ›ia ei Ã®n labirint. Folosind find, totodata
fiind indrumata de informatiile furnizate in acest link https://www.mathworks.com/matlabcentral/answers/214388-how-do-i-find-what-row-and-column-a-specific-element-is-in
identificÄƒm poziÈ›ia fiecÄƒrei stÄƒri din path, ignorÃ¢nd starea finalÄƒ WIN.
Se construieÈ™te astfel decoded_path, vectorul de coordonate ce descrie vizual
parcursul Ã®n labirint.

### Task 2. Linear Regression

1. Functia parse_data_set_file:
	Am citit prima linie a fisierului cu fgetl si de acolo am extras
dimensiunile matricii. Ulterior, am creat trei matrici, doua de tip [cell]
(https://www.mathworks.com/help/matlab/ref/cell.html) si una de tip numeric.
Prima matrice se stocheaza cu toate informatiile din fisier, acestea fiind
prelucrate cu ajutorul functiei [regexp]
(https://www.mathworks.com/help/matlab/ref/regexp.html).
Prima coloana a matricii A este atribuita lui Y, datele fiind convertite cu
ajutorul functiei [str2double]
(https://www.mathworks.com/help/matlab/ref/str2double.html), restul fiind
atribuit in InitialMatrix.

2. Functia parse_csv_file:
	Am citit prima linie a fisierului pentru a duce cursorul incepand de la
linia a doua a acestuia. Cu [textscan]
(https://www.mathworks.com/help/matlab/ref/textscan.html) am extras continutul
ramas necitit, iar apoi am stocat in A toata informatia prelucrata cu regexp.
Prima coloana a matricii A s-a dus in Y, iar restul in InitialMatrix.

3. Functia prepare_for_regression:
	Am creat o matrice numerica in care voi adauga toate informatiile prelucrate
din InitialMatrix. Astfel, am creat doua for-uri pentru a ma plimba prin matrice
element cu element. Compar elementele din InitialMatrix cu [strcmp]
(https://www.mathworks.com/help/matlab/ref/strcmp.html) Am creat un contor a
care este 0 pe fiecare linie iar cand ajunge la coloana n-1 in cazul in care
avem de modificat 'furnished', 'semi-furnished' sau 'unfurnished' acesta se
face 1. In acest fel, putem umple corespunzator matricea FeatureMatrix.

4. Functia linear_regression_cost_function:
	Am calculat functia de cost cu formula din suportul teoretic aferent task-
ului. Astfel, am extras prima coloana din Theta pentru ca aceasta era 0 si nu
ne ajuta la calculul valorii prezise. Totodata, am scazut iesirile, am ridicat
la patrat fiecare element din vectorul rezultat, am adunat apoi termenii sai
iar la final am calculat eroarea.

5. Functia gradient_descent:
	Am adaugat o coloana de 1 atat in Theta cat si FutureMatrix cu ajutorul
functiei [cat] (https://www.mathworks.com/help/matlab/ref/double.cat.html),
pentru a permite calculul corect al produsului Ã®ntre matrice. Ãn cadrul
algoritmului, se calculeazÄƒ eroarea Ã®ntre predicÈ›iile modelului È™i valorile
reale, iar coeficientul Theta este actualizat iterativ, Ã®ncepÃ¢nd de la al doilea
element pentru a ignora influenÈ›a valorii 0 a primului element.

6. Functia normal_equation:
	Ãn aceastÄƒ funcÈ›ie, folosim metoda ecuaÈ›iei normale combinatÄƒ cu gradientul
conjugat pentru a calcula vectorul de parametri Theta. Se adaugÄƒ o coloanÄƒ de 1
la matricea de intrare pentru a putea include termenul liber. Se calculeazÄƒ
matricea A È™i vectorul b necesare rezolvÄƒrii sistemului normal de ecuaÈ›ii.
Pentru a evita modificarea primei componente a lui Theta, sistemul este
restricÈ›ionat doar la coeficienÈ›ii corespunzÄƒtori caracteristicilor. Apoi,
folosind gradientul conjugat, se actualizeazÄƒ pas cu pas valorile din Theta pÃ¢nÄƒ
cÃ¢nd eroarea devine mai micÄƒ decÃ¢t o toleranÈ›Äƒ datÄƒ sau pÃ¢nÄƒ la un numÄƒr maxim
de iteraÈ›ii.

7. Functia lasso_regresion_cost_function:
	Am folosit aceleasi calcule ca la linear_regression_cost_function, iar
apoi am calculat regularizarea prin adaugarea tuturor elementelor in modul.
Am calculat ulterior eroarea cu formula specifica acestui caz.

8. Functia ridge_regression_cost_function:
	Am folosit aceleasi calcule ca la linear_regression_cost_function, iar
apoi am calculat regularizarea prin adaugarea tuturor elementelor la patrat.
Am calculat ulterior eroarea cu formula specifica acestui caz.

### Task 3. MNIST 101

1. Functia load_dataset:
	Am extras toate informatiile din fisierul de tip .mat cu ajutorul functiei
[load] (https://www.mathworks.com/help/matlab/ref/load.html).

2. Functia split_dataset:
	Am permutat liniile cu [randperm]
(https://www.mathworks.com/help/matlab/ref/randperm.html) si am aplicat aceeasi
permutare pe X si y. Am calculat cate teste se duc in train, respectiv cate se
duc in test. Ulterior, am adaugat primele m_train teste in setul de train, iar
pe restul le am adaugat in setul de test. Ideea de a permuta matricea si
vectorul in acelasi fel am luat-o din acest link:
(https://www.mathworks.com/matlabcentral/answers/1568213-how-to-shuffle-two-matrices-in-a-consistent-manner)

3. Functia initialize_weights:
	Am adunat cele doua dimensiuni si l-am calculat pe epsilon, apoi am calculat
o matrice cu valori aleatoare intre -epsilon si epsilon astfel: cum stim ca
functia [rand] (https://www.mathworks.com/help/matlab/ref/rand.html) ne da o
matrice cu valori intre 0 si 1, am inmultit cu 2*epsilon
pentru a obtine una cu valori in 0, 2*epsilon, iar la final am scazut un epsilon
pentru a avea in matrice valori intre -epsilon si epsilon.

4. Functia cost_function:
	Am Ã®mpÄƒrÈ›it vectorul params Ã®n matricile Theta1 È™i Theta2 folosind reshape.
Am adÄƒugat o coloanÄƒ de 1 la X È™i am aplicat propagarea Ã®nainte, calculÃ¢nd
activÄƒrile a2 È™i a3 prin funcÈ›ia sigmoid. Costul a fost calculat folosind
formula specificÄƒ reÈ›elelor neuronale, iar apoi a fost Ã®mpÄƒrÈ›it la numÄƒrul de
exemple È™i a fost adÄƒugatÄƒ regularizarea fÄƒrÄƒ a afecta coloana de bias. Am
aplicat apoi backpropagation pentru a calcula gradientele triunghi1 È™i
triunghi2. Gradientele au fost ajustate cu regularizare È™i Ã®n final au fost
concatenate Ã®ntr-un vector coloanÄƒ pentru rezultat.

5. Functia predict_classes:
	Ãn aceastÄƒ funcÈ›ie, folosind dimensiunile de intrare, reconstruim matricile
de ponderi pentru straturile reÈ›elei. Se adaugÄƒ o coloanÄƒ de 1 Ã®n faÈ›a lui X
pentru bias, iar apoi se calculeazÄƒ activÄƒrile pentru stratul ascuns È™i stratul
de ieÈ™ire prin Ã®nmulÈ›iri succesive È™i aplicarea funcÈ›iei sigmoid. DupÄƒ obÈ›inerea
activÄƒrilor finale, pentru fiecare exemplu se parcurg valorile din stratul de
ieÈ™ire È™i se cautÄƒ maximul. PoziÈ›ia acestuia este consideratÄƒ clasa prezisÄƒ È™i
este stocatÄƒ Ã®ntr-un vector rezultat.
